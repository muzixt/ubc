{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-23T02:04:45.175415900Z",
     "start_time": "2023-12-23T02:04:45.124879200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2 as cv\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "root = Path('/home/cqwu/lw/UBC')\n",
    "test_ann = root / \"test.csv\"\n",
    "test_dir = root / \"test_thumbnails\"\n",
    "tma_test_dir = root / \"test_images\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T02:04:45.884426700Z",
     "start_time": "2023-12-23T02:04:45.876438700Z"
    }
   },
   "id": "915b4bbc9c1f2989"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def crop_image_ratio(image):\n",
    "    height, width, _ = image.shape\n",
    "    aspect_ratio = float(width) / height\n",
    "    inverse_aspect_ratio = float(height) / width\n",
    "    if aspect_ratio < 1.5 and inverse_aspect_ratio < 1.5:\n",
    "        new_width = width // 2\n",
    "        new_height = height // 2\n",
    "        cropped_images = [\n",
    "            image[:new_height, :new_width],\n",
    "            image[:new_height, new_width:],\n",
    "            image[new_height:, :new_width],\n",
    "            image[new_height:, new_width:]\n",
    "        ]\n",
    "    elif aspect_ratio >= 1.5:\n",
    "        num_crops = math.ceil(aspect_ratio)\n",
    "        crop_width = width // num_crops\n",
    "        cropped_images = []\n",
    "        for i in range(num_crops):\n",
    "            start_col = i * crop_width\n",
    "            end_col = start_col + crop_width\n",
    "            cropped_image = image[:, start_col:end_col]\n",
    "            cropped_images.append(cropped_image)\n",
    "    elif inverse_aspect_ratio >= 1.5:\n",
    "        num_crops = math.ceil(inverse_aspect_ratio)\n",
    "        crop_height = height // num_crops\n",
    "        cropped_images = []\n",
    "        for i in range(num_crops):\n",
    "            start_row = i * crop_height\n",
    "            end_row = start_row + crop_height\n",
    "            cropped_image = image[start_row:end_row, :]\n",
    "            cropped_images.append(cropped_image)\n",
    "\n",
    "    return cropped_images\n",
    "\n",
    "\n",
    "# def crop_image(image):\n",
    "#     min_area = image.shape[0] * image.shape[1] * 0.07\n",
    "#     gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "#     _, binary = cv.threshold(gray, 127, 255, cv.THRESH_BINARY)\n",
    "#     contours, _ = cv.findContours(binary, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "#     del gray, binary\n",
    "#     output_images = []\n",
    "#     for contour in contours:\n",
    "#         x, y, w, h = cv.boundingRect(contour)\n",
    "#         area = w * h\n",
    "#         if area >= min_area:\n",
    "#             output_images.append(image[y:y + h, x:x + w].copy())\n",
    "#     del image\n",
    "#     return output_images\n",
    "\n",
    "def crop_image(image, min_factor=0.1):\n",
    "    cropped_images = []\n",
    "    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    _, binary = cv.threshold(gray, 127, 255, cv.THRESH_BINARY)\n",
    "    horizontal_projection = np.sum(binary, axis=1)\n",
    "    vertical_projection = np.sum(binary, axis=0)\n",
    "    horizontal_index = np.nonzero(horizontal_projection)[0]\n",
    "    vertical_index = np.nonzero(vertical_projection)[0]\n",
    "\n",
    "    if horizontal_index.size != horizontal_projection.size or vertical_index.size != vertical_projection.size:\n",
    "        y_ranges = find_discontinuous_ranges(horizontal_index)\n",
    "        x_ranges = find_discontinuous_ranges(vertical_index)\n",
    "        for y_range in y_ranges:\n",
    "            if y_range[1] - y_range[0] + 1 >= min_factor * image.shape[0]:\n",
    "                for x_range in x_ranges:\n",
    "                    if x_range[1] - x_range[0] + 1 >= min_factor * image.shape[1]:\n",
    "                        cropped_images.append(image[y_range[0]:y_range[1] + 1, x_range[0]:x_range[1] + 1].copy())\n",
    "    else:\n",
    "        cropped_images.append(image)\n",
    "    return cropped_images\n",
    "\n",
    "\n",
    "def find_discontinuous_ranges(lst):\n",
    "    # input [1, 2, 3, 6, 7, 9, 10, 13, 14]\n",
    "    # output [(1, 3), (6, 7), (9, 10), (13, 14)]\n",
    "    diff = np.diff(lst)\n",
    "    boundaries = np.where(diff != 1)[0] + 1\n",
    "    start_values = np.insert(lst[boundaries], 0, lst[0])\n",
    "    end_values = np.append(lst[boundaries - 1], lst[-1])\n",
    "    ranges = list(zip(start_values, end_values))\n",
    "    return ranges\n",
    "\n",
    "\n",
    "def get_img_path(image_id):\n",
    "    path = test_dir / f\"{image_id}_thumbnail.png\"\n",
    "    if path.exists():\n",
    "        return path\n",
    "    else:\n",
    "        return tma_test_dir / f\"{image_id}.png\"\n",
    "\n",
    "\n",
    "def resize_short_edge(image, target_size=512):\n",
    "    height, width = image.shape[:2]\n",
    "    aspect_ratio = float(target_size) / min(height, width)\n",
    "    new_height = int(round(height * aspect_ratio))\n",
    "    new_width = int(round(width * aspect_ratio))\n",
    "    resized_image = cv.resize(image, (new_width, new_height))\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "class CancerThumbnailDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df_data, img_root_dir, transforms=None, split: float = 0.90):\n",
    "        self.split = split\n",
    "        self.img_root_dir = img_root_dir\n",
    "        self.transforms = None\n",
    "        self.data = df_data\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.imgs = [get_img_path(id) for id in self.data[\"image_id\"]]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        img = cv.imread(str(self.imgs[idx]))\n",
    "        # img = cv.resize(img, None, fx=1 / 3, fy=1 / 3)\n",
    "        resize_short_edge(img, target_size=512)\n",
    "        ims = crop_image(img)\n",
    "        imms = []\n",
    "        images = []\n",
    "        for m in ims:\n",
    "            imms += crop_image_ratio(m)\n",
    "\n",
    "        for img in imms:\n",
    "            img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "            if self.transforms:\n",
    "                c_img = self.transforms(image=img)['image']\n",
    "                images.append(c_img)\n",
    "\n",
    "        return images\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T02:04:46.500601Z",
     "start_time": "2023-12-23T02:04:46.475631600Z"
    }
   },
   "id": "2ad2bff2aefdd79d"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = models.resnext50_32x4d()\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 500)\n",
    "        self.fc = nn.Linear(500, 100)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc1 = nn.Linear(100, num_classes)\n",
    "        self.sf = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.fc1(x)\n",
    "        # x = self.sf(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T02:09:54.040392600Z",
     "start_time": "2023-12-23T02:09:53.977354Z"
    }
   },
   "id": "a824fde6c2fd4f63"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "===============================================================================================\nLayer (type:depth-idx)                        Output Shape              Param #\n===============================================================================================\nNet                                           [1, 5]                    --\n├─ResNet: 1-1                                 [1, 500]                  --\n│    └─Conv2d: 2-1                            [1, 64, 112, 112]         9,408\n│    └─BatchNorm2d: 2-2                       [1, 64, 112, 112]         128\n│    └─ReLU: 2-3                              [1, 64, 112, 112]         --\n│    └─MaxPool2d: 2-4                         [1, 64, 56, 56]           --\n│    └─Sequential: 2-5                        [1, 256, 56, 56]          --\n│    │    └─Bottleneck: 3-1                   [1, 256, 56, 56]          63,488\n│    │    └─Bottleneck: 3-2                   [1, 256, 56, 56]          71,168\n│    │    └─Bottleneck: 3-3                   [1, 256, 56, 56]          71,168\n│    └─Sequential: 2-6                        [1, 512, 28, 28]          --\n│    │    └─Bottleneck: 3-4                   [1, 512, 28, 28]          349,184\n│    │    └─Bottleneck: 3-5                   [1, 512, 28, 28]          282,624\n│    │    └─Bottleneck: 3-6                   [1, 512, 28, 28]          282,624\n│    │    └─Bottleneck: 3-7                   [1, 512, 28, 28]          282,624\n│    └─Sequential: 2-7                        [1, 1024, 14, 14]         --\n│    │    └─Bottleneck: 3-8                   [1, 1024, 14, 14]         1,390,592\n│    │    └─Bottleneck: 3-9                   [1, 1024, 14, 14]         1,126,400\n│    │    └─Bottleneck: 3-10                  [1, 1024, 14, 14]         1,126,400\n│    │    └─Bottleneck: 3-11                  [1, 1024, 14, 14]         1,126,400\n│    │    └─Bottleneck: 3-12                  [1, 1024, 14, 14]         1,126,400\n│    │    └─Bottleneck: 3-13                  [1, 1024, 14, 14]         1,126,400\n│    └─Sequential: 2-8                        [1, 2048, 7, 7]           --\n│    │    └─Bottleneck: 3-14                  [1, 2048, 7, 7]           5,550,080\n│    │    └─Bottleneck: 3-15                  [1, 2048, 7, 7]           4,497,408\n│    │    └─Bottleneck: 3-16                  [1, 2048, 7, 7]           4,497,408\n│    └─AdaptiveAvgPool2d: 2-9                 [1, 2048, 1, 1]           --\n│    └─Linear: 2-10                           [1, 500]                  1,024,500\n├─Dropout: 1-2                                [1, 500]                  --\n├─Linear: 1-3                                 [1, 100]                  50,100\n├─Linear: 1-4                                 [1, 5]                    505\n===============================================================================================\nTotal params: 24,055,009\nTrainable params: 24,055,009\nNon-trainable params: 0\nTotal mult-adds (G): 4.23\n===============================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 230.41\nParams size (MB): 96.22\nEstimated Total Size (MB): 327.24\n==============================================================================================="
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "mol=Net()\n",
    "summary(mol, input_size=(1,3, 224, 224))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T02:10:19.224017Z",
     "start_time": "2023-12-23T02:10:18.311846200Z"
    }
   },
   "id": "a3d07d964529a34f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "bs = 1\n",
    "num_workers = 4\n",
    "df_test_data = pd.read_csv(test_ann)\n",
    "test_transforms = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "test_dataset = CancerThumbnailDataset(df_test_data, img_root_dir=test_dir, transforms=test_transforms)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T09:05:11.295207600Z",
     "start_time": "2023-12-22T09:05:11.295207600Z"
    }
   },
   "id": "c9fcabf41058c92c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size: torch.Size([2, 3, 224, 224])\n",
      "image size: torch.Size([4, 3, 224, 224])\n",
      "image size: torch.Size([4, 3, 224, 224])\n",
      "image size: torch.Size([4, 3, 224, 224])\n",
      "image size: torch.Size([8, 3, 224, 224])\n",
      "image size: torch.Size([4, 3, 224, 224])\n",
      "image size: torch.Size([4, 3, 224, 224])\n",
      "image size: torch.Size([12, 3, 224, 224])\n",
      "image size: torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for imgs in test_dataloader:\n",
    "    print(f'image size: {torch.cat(imgs).shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T09:05:15.660296400Z",
     "start_time": "2023-12-22T09:05:11.295207600Z"
    }
   },
   "id": "ffd997d5584aa07d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "\n",
    "model_path = r'/home/cqwu/lw/UBC/logs/ubc/version_10/checkpoints/best-model-epoch=142-val_acc=0.93.ckpt'\n",
    "state = torch.load(model_path)\n",
    "model.load_state_dict(state['state_dict'], strict=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T09:05:17.271577500Z",
     "start_time": "2023-12-22T09:05:15.655299800Z"
    }
   },
   "id": "cfce9f763a9a6cbf"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9\n",
      "torch.Size([9, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": "[2, 2, 3, 2, 2, 1, 2, 2, 2]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @torch.no_grad()\n",
    "# def predict(model, dl):\n",
    "#     model.eval()\n",
    "#     pp = []\n",
    "#     for i, t in enumerate(dl):\n",
    "#         p = model(t.to(device))\n",
    "#         pp.append(p)\n",
    "#         print(\"\\r{}/{}\".format(i + 1, len(dl) // bs), end=\"\")\n",
    "#     print()\n",
    "#     result = torch.cat(pp)\n",
    "#     conf, cls = result.topk(1, 1)\n",
    "#     conf = conf.squeeze(1).cpu().numpy().tolist()\n",
    "#     cls = cls.squeeze(1).cpu().numpy().tolist()\n",
    "#     # 可选\n",
    "#     cls = [cs if c > 0.6 else 5 for cs, c in zip(cls, conf)]\n",
    "#     return cls\n",
    "@torch.no_grad()\n",
    "def predict(model, dl):\n",
    "    model.eval()\n",
    "    pp = []\n",
    "    for i, t in enumerate(dl):\n",
    "        p = model(torch.cat(t).to(device))\n",
    "        sp = torch.sum(p, dim=0)\n",
    "        pp.append(sp)\n",
    "        print(\"\\r{}/{}\".format(i + 1, len(dl) // bs), end=\"\")\n",
    "    print()\n",
    "    result = torch.stack(pp, dim=0)\n",
    "    print(result.shape)\n",
    "    conf, cls = result.topk(1, 1)\n",
    "    conf = conf.squeeze(1).cpu().numpy().tolist()\n",
    "    cls = cls.squeeze(1).cpu().numpy().tolist()\n",
    "    # 可选\n",
    "    cls = [cs if c > 0.6 else 5 for cs, c in zip(cls, conf)]\n",
    "    return cls\n",
    "\n",
    "\n",
    "res = predict(model, test_dataloader)\n",
    "# 2 2 3 2 3 1 2 2 2\n",
    "display(res)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T09:05:22.734416900Z",
     "start_time": "2023-12-22T09:05:17.274557500Z"
    }
   },
   "id": "aeb9aa586481eec5"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "   image_id  image_width  image_height label\n0        41        28469         16987  HGSC\n1         4        23785         20008  HGSC\n2        66        48871         48195  LGSC\n3        91         3388          3388  HGSC\n4       281        42309         15545  HGSC\n5       286        37204         30020    EC\n6       431        39991         40943  HGSC\n7       706        75606         25965  HGSC\n8       970        32131         18935  HGSC",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>image_width</th>\n      <th>image_height</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41</td>\n      <td>28469</td>\n      <td>16987</td>\n      <td>HGSC</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>23785</td>\n      <td>20008</td>\n      <td>HGSC</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66</td>\n      <td>48871</td>\n      <td>48195</td>\n      <td>LGSC</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>91</td>\n      <td>3388</td>\n      <td>3388</td>\n      <td>HGSC</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>281</td>\n      <td>42309</td>\n      <td>15545</td>\n      <td>HGSC</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>286</td>\n      <td>37204</td>\n      <td>30020</td>\n      <td>EC</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>431</td>\n      <td>39991</td>\n      <td>40943</td>\n      <td>HGSC</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>706</td>\n      <td>75606</td>\n      <td>25965</td>\n      <td>HGSC</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>970</td>\n      <td>32131</td>\n      <td>18935</td>\n      <td>HGSC</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['CC', 'EC', 'HGSC', 'LGSC', 'MC', 'Other']\n",
    "label_list = list(map(lambda x: labels[x], res))\n",
    "df_test_data['label'] = label_list\n",
    "display(df_test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T09:05:22.752416600Z",
     "start_time": "2023-12-22T09:05:22.732421200Z"
    }
   },
   "id": "8768cc7e03911aff"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df_test_data[[\"image_id\", \"label\"]].to_csv(\"submission.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T09:05:22.767418600Z",
     "start_time": "2023-12-22T09:05:22.748416700Z"
    }
   },
   "id": "12fdc92d7d5679cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
